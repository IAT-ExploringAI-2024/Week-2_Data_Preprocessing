{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX1FMIN9qkMIBUWc7DH6PR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackylmw/Week-2_Data_Preprocessing/blob/Assignment2_JackyLam/JackyLam_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import drive for subsequent applications"
      ],
      "metadata": {
        "id": "5zPfH43qQW5k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQdQderjQNZh",
        "outputId": "1ce8c967-f942-42f2-c51d-6a31d92ed25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required libraries"
      ],
      "metadata": {
        "id": "qvu5tUsMRLyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import soundfile"
      ],
      "metadata": {
        "id": "329w02T5RUVG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge EmoDB and RAVDESS\n",
        "\n",
        "The EMODB dataset includes recordings from ten professional voice artists, equally divided between males and females.  Each artist in this dataset expresses seven different emotions.  Similarly, the RAVDESS dataset contains recordings from 24 professional actors, showcasing eight distinct emotional expressions.  Although both datasets include similar types of recordings, EmoDB and RAVDESS each use a completely different system for naming their files.  The challenge I encountered was to seamlessly integrate the EMODB recordings into the RAVDESS collection, for which I adjusted the naming scheme of the EMODB files to align with RAVDESS's system.\n"
      ],
      "metadata": {
        "id": "xN7IKNZhRwsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actor's ID in EmoDB dataset will be represented using the representation in the RAVDESS dataset"
      ],
      "metadata": {
        "id": "q2_MYg45TYDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actor_id = {\n",
        "    \"03\": \"25\",\n",
        "    \"08\": \"26\",\n",
        "    \"10\": \"27\",\n",
        "    \"09\": \"28\",\n",
        "    \"11\": \"29\",\n",
        "    \"13\": \"30\",\n",
        "    \"12\": \"31\",\n",
        "    \"14\": \"32\",\n",
        "    \"15\": \"33\",\n",
        "    \"16\": \"34\",\n",
        "}"
      ],
      "metadata": {
        "id": "PVMilFoZSqfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the emotion categories in the EmoDB dataset are not exactly consistent with those in the RAVDESS dataset, I think bored in EmoDB is calm in RAVDESS. Since the EmoDB does not contain the emotion of surprised, I left it alone, which meant keeping the emotion number of surprised in the original RAVDESS dataset."
      ],
      "metadata": {
        "id": "8zeXHahyUIIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion = {\"W\": \"05\",\n",
        "           \"E\": \"07\",\n",
        "           \"A\": \"06\",\n",
        "           \"F\": \"03\",\n",
        "           \"T\": \"04\",\n",
        "           \"N\": \"01\",\n",
        "           \"L\": \"02\"}"
      ],
      "metadata": {
        "id": "oSjKwrpnVTfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the EmoDB dataset, different versions are represented by letters, while in RAVDESS, different versions are represented by numbers. To make the EmoDB consistent with RAVDESS naming, I will convert letters to numbers."
      ],
      "metadata": {
        "id": "GnACVoT4XN8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "versions = {\"a\": \"01\", \"b\": \"02\", \"c\": \"03\", \"d\": \"04\", \"e\": \"05\", \"f\": \"06\"}"
      ],
      "metadata": {
        "id": "g0XbUZW2XSNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renaming every file in the EmoDB dataset is a tedious step. Completing this task manually would be a daunting and tedious endeavor. Therefore, I developed a Python script to speed up the process. This script will rename each file in the EmoDB dataset. If no RAVDESS folder exists in the same location as the EmoDB folder, a new RAVDESS directory will be generated. If there is already a RAVDESS folder, there is no need to regenerate and overwrite the original folder. Executing this script directly renames the files in the EmoDB folder and transfers them to the RAVDESS folder."
      ],
      "metadata": {
        "id": "l3PGBxOr3qv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir(os.path.dirname(\"/content/drive/MyDrive/IAT481-Assignment2/wav\"))\n",
        "\n",
        "if not os.path.exists(\"./RAVDESS\"):\n",
        "    os.mkdir(\"./RAVDESS\")\n",
        "\n",
        "files = os.listdir(\"./wav\")\n",
        "\n",
        "for f in files:\n",
        "    new_f = (\n",
        "        \"03-01-\"\n",
        "        + emotion[f[5]]\n",
        "        + \"-01-\"\n",
        "        + f[2:5]\n",
        "        + \"-\"\n",
        "        + versions[f[6]]\n",
        "        + \"-\"\n",
        "        + actor_id[f[0:2]]\n",
        "        + \".wav\"\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(\"./RAVDESS/Actor_\" + actor_id[f[0:2]]):\n",
        "        os.mkdir(\"./RAVDESS/Actor_\" + actor_id[f[0:2]])\n",
        "\n",
        "\n",
        "    os.rename(\"./wav/\" + f, \"./RAVDESS/Actor_\" + actor_id[f[0:2]] + \"/\" + new_f)"
      ],
      "metadata": {
        "id": "C_o8oewLXeIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis\n",
        "\n",
        "In this section, we need to visualize our combined dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "EJhPDW9pJzay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Dataset and Compute Features**\n",
        "\n",
        "We have to understand the labelling of the RAVDESS dataset to find the ground truth emotion for each sample. Each file is labelled with 7 numbers delimited by a \"-\". Most of the numbers describe metadata about the audio samples such as their format (video and/or audio), whether the audio is a song or statement, which of two statements is being read and by which actor.\n"
      ],
      "metadata": {
        "id": "Ek5BdfxBQc6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to define a dictionary based on the third number (emotion) and assign an emotion to each number as specified by the RAVDESS dataset:"
      ],
      "metadata": {
        "id": "1K-SuwxQJrT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Emotions in the RAVDESS dataset\n",
        "emotions ={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}"
      ],
      "metadata": {
        "id": "-LjhAbYDQ8_W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "def feature_chromagram(waveform, sample_rate):\n",
        "    # STFT computed here explicitly; mel spectrogram and MFCC functions do this under the hood\n",
        "    stft_spectrogram=np.abs(librosa.stft(waveform))\n",
        "    #print(stft_spectrogram.shape)\n",
        "    # Produce the chromagram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    chromagram=np.mean(librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate).T,axis=0)\n",
        "    #print(chromagram.shape)\n",
        "    return chromagram\n",
        "\n",
        "def feature_melspectrogram(waveform, sample_rate):\n",
        "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
        "    melspectrogram=np.mean(librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128, fmax=8000).T,axis=0)\n",
        "    return melspectrogram\n",
        "\n",
        "def feature_mfcc(waveform, sample_rate):\n",
        "    # Compute the MFCCs for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
        "    # 40 filterbanks = 40 coefficients\n",
        "    mfc_coefficients=np.mean(librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "    return mfc_coefficients"
      ],
      "metadata": {
        "id": "2JpyXBJxSezN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(file):\n",
        "    # load an individual soundfile\n",
        "     with soundfile.SoundFile(file) as audio:\n",
        "        waveform = audio.read(dtype=\"float32\")\n",
        "        sample_rate = audio.samplerate\n",
        "        # compute features of soundfile\n",
        "        chromagram = feature_chromagram(waveform, sample_rate)\n",
        "        melspectrogram = feature_melspectrogram(waveform, sample_rate)\n",
        "        mfc_coefficients = feature_mfcc(waveform, sample_rate)\n",
        "        feature_matrix=np.array([])\n",
        "\n",
        "        # use np.hstack to stack our feature arrays horizontally to create a feature matrix\n",
        "        feature_matrix = np.hstack((chromagram, melspectrogram, mfc_coefficients))\n",
        "\n",
        "        return feature_matrix"
      ],
      "metadata": {
        "id": "e5BB7nkbRoYD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DEP35N3gSc_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "def load_data():\n",
        "    X,y=[],[]\n",
        "    count = 0\n",
        "    for file in glob.glob(\"/content/drive/MyDrive/IAT481-Assignment2/audio_speech_actors_01-24/Actor_*/*.wav\"):\n",
        "        file_name=os.path.basename(file)\n",
        "        #print(file_name)\n",
        "        #print(file_name.split(\"-\")[2])\n",
        "        emotion=emotions[(file_name.split(\"-\")[2])]\n",
        "        #print(emotion)\n",
        "        features = get_features(file)\n",
        "        X.append(features)\n",
        "        y.append(emotion)\n",
        "        count += 1\n",
        "        # '\\r' + end='' results in printing over same line\n",
        "        print('\\r' + f' Processed {count}/{1435} audio samples',end=' ')\n",
        "      # Return arrays to plug into sklearn's cross-validation algorithms\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "RiJ4dryaRK41"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, emotions = load_data()"
      ],
      "metadata": {
        "id": "9HRalL65RZO2",
        "outputId": "ae6f1d59-7e50-4e1a-b61e-ca96754d238c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processed 517/1435 audio samples "
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 12 and the array at index 1 has size 128",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-916fe6a19769>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-58904cf8e95b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0memotion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(emotion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d08e7a62da9f>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# use np.hstack to stack our feature arrays horizontally to create a feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfeature_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromagram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmelspectrogram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmfc_coefficients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 12 and the array at index 1 has size 128"
          ]
        }
      ]
    }
  ]
}